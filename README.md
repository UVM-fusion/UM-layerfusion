# Towards Maximizing the Sweet Spot for NLP Models
This repo is the final project for the [2024] Deep Learning &amp; NLP Final Project course. 

## ðŸ“‹Overview
Large NLP models face significant memory constraints, making it challenging to execute them effectively due to their increasing memory demands. To tackle this, we integrate **layer fusion** with **unified memory**â€”a method that allows programs to utilize more memory than typically available. This approach seeks to balance memory efficiency and performance, enabling large Transformer models to run seamlessly on a single processor with minimal performance degradation. The project evaluates the effectiveness of this strategy in supporting large-scale NLP models.


## ðŸ› Â Environment Setup



## ðŸ‘ŸÂ Run Experiments



## ðŸ“Š Experiment Results



## ðŸŒŸÂ Project members (Team09)

| <img width="200" src="https://user-images.githubusercontent.com/68412683/206727359-a653906e-0847-4702-a7e4-4c1ac532bd46.png"/> | <img width="200" src="https://user-images.githubusercontent.com/68412683/206727359-a653906e-0847-4702-a7e4-4c1ac532bd46.png"/> |
| --- | --- |
| **Ji Yeong Yi** | **Jane Rhee** |
| AIX 4th | AIX 5th |
| jybyte@gmail.com | jrhee1122@ewhain.net |
